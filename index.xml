<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>My Big Data Collection</title>
    <link>https://omennemo.github.io/tutorial/</link>
    <description>Recent content on My Big Data Collection</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Apr 2017 00:56:37 +0530</lastBuildDate>
    
	<atom:link href="https://omennemo.github.io/tutorial/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Installing Java on Ubuntu Guest</title>
      <link>https://omennemo.github.io/tutorial/post/install-java/</link>
      <pubDate>Thu, 13 Apr 2017 00:56:37 +0530</pubDate>
      
      <guid>https://omennemo.github.io/tutorial/post/install-java/</guid>
      <description>INSTALL VMWare workstation Search VM Ware workstation for windows and install. If required enable virtualization in BIOS to enable your laptop or desktop for VM.
INSTALL UBUNTU Search for the ISO file for Ubuntu 16.04 from internet and download iso image (64 bit). Preferably 16.04 (16.10 has some issue)
JAVA Installation Step 1:- Google Search “Java 8 download” in the internet and download 64&amp;frasl;32 bit java ( tar.gz ) You are in ~/Downloads directory</description>
    </item>
    
    <item>
      <title>Installing Hadoop on the Ubuntu Guest</title>
      <link>https://omennemo.github.io/tutorial/post/install-hadoop/</link>
      <pubDate>Thu, 13 Apr 2017 11:12:24 +0530</pubDate>
      
      <guid>https://omennemo.github.io/tutorial/post/install-hadoop/</guid>
      <description>Configure User  Login as Root:
sudo su
whoami &amp;ndash; should give root
 Adding a dedicated Hadoop system user called “hduser”
We will use a dedicated Hadoop user account for running Hadoop. While that’s not required it is recommended because it helps to separate the Hadoop installation from other software applications and user accounts running on the same machines (this: security, permission and backups etc.)
 Create a group called hadoop</description>
    </item>
    
    <item>
      <title>Installing a multinode cluster</title>
      <link>https://omennemo.github.io/tutorial/post/install-multinode/</link>
      <pubDate>Thu, 13 Apr 2017 14:06:21 +0530</pubDate>
      
      <guid>https://omennemo.github.io/tutorial/post/install-multinode/</guid>
      <description>Create machines  Create 3 nodes Load VMs (NAME: master , datanode1, datanode2) Check if nodes are ping-able (with IP address – use command ifconfig to find IP Address) Change the hostname
sudo nano /etc/hostname
Replace with (master, datanode1, datanode2)
 Update host file and update with IP address
sudo nano /etc/hosts
Delete Ubuntu line and add these with the appropriate IP addresses
192.168.72.X master 192.168.72.Y datanode1 192.168.72.Z datanode2  Reboot</description>
    </item>
    
    <item>
      <title>Installing Hive</title>
      <link>https://omennemo.github.io/tutorial/post/install-hive/</link>
      <pubDate>Thu, 13 Apr 2017 14:31:46 +0530</pubDate>
      
      <guid>https://omennemo.github.io/tutorial/post/install-hive/</guid>
      <description>Guide to installing Hive on a hadoop cluster  Create a folder called ecosystem at ~
mkdir ecosystem
cd ecosystem
 Get the Hive installation file
wget http://mirror.fibergrid.in/apache/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz
If this command doesn&amp;rsquo;t work, please search for the installation zip file and use the same command to download the file.
 Un-tar the file
tar -xvzf apache-hive-2.1.1-bin.tar.gz
 Remove the tar file once the file is extracted properly
rm apache-hive-2.1.1-bin.tar.gz</description>
    </item>
    
  </channel>
</rss>