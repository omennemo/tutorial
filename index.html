<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.20" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  
  <title>Big Data How-To</title>
  

  
  <link rel="stylesheet" href="https://omennemo.github.io/tutorial/css/poole.css">
  <link rel="stylesheet" href="https://omennemo.github.io/tutorial/css/syntax.css">
  <link rel="stylesheet" href="https://omennemo.github.io/tutorial/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://omennemo.github.io/tutorial/index.xml" rel="alternate" type="application/rss+xml" title="Big Data How-To" />
</head>

<body class="theme-base-0d ">

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://omennemo.github.io/tutorial/"><h1>Big Data How-To</h1></a>
      <p class="lead">
      An elegant open source and mobile first theme for <a href="http://hugo.spf13.com">hugo</a> made by <a href="http://twitter.com/mdo">@mdo</a>. Originally made for Jekyll.
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="/">Home</a> </li>
      
    </ul>

    <p>&copy; 2017. All rights reserved. </p>
  </div>
</div>


    <div class="content container">
<div class="posts">

      
  <div class="post">
    <h1 class="post-title">
      <a href="https://omennemo.github.io/tutorial/post/install-java/">
        Installing Java on Ubuntu Guest
      </a>
    </h1>

    <span class="post-date">Thu, Apr 13, 2017</span>

    

<h2 id="install-vmware-workstation"><strong>INSTALL VMWare workstation</strong></h2>

<p>Search VM Ware workstation for windows and install. If required enable virtualization in BIOS to enable your laptop or desktop for VM.</p>

<h2 id="install-ubuntu"><strong>INSTALL UBUNTU</strong></h2>

<p>Search for the ISO file for Ubuntu 16.04 from internet and download iso image (64 bit). Preferably 16.04 (16.10 has some issue)</p>

<h2 id="java-installation"><strong>JAVA Installation</strong></h2>

<p>Step 1:- Google Search “Java 8 download” in the internet and download <sup>64</sup>&frasl;<sub>32</sub> bit java ( tar.gz )
You are in ~/Downloads directory</p>

<p>Step 2:</p>

<p><code>sudo cp jdk-8u121-linux-x64.tar.gz /usr/local</code></p>

<p>(coping tar file from Downloads directory to /usr/local/.)</p>

<p><code>cd /usr/local</code></p>

<p>Step 3:-</p>

<p><code>sudo tar -xvzf jdk-8u121-linux-x64.tar.gz</code></p>

<p>Step 4:-</p>

<p><code>sudo rm jdk-8u121-linux-x64.tar.gz</code></p>

<p>Step 5:-</p>

<p><code>sudo ln -s jdk1.8.0_121 java</code></p>

<p>(rm -r java if symbolic link error)</p>

<p><code>sudo chmod 777 jdk1.8.0_121</code></p>

<p>Step 6:-</p>

<p><code>sudo update-alternatives --install ''/usr/bin/java&quot; &quot;java&quot; &quot;/usr/local/java/bin/java&quot; 1</code></p>

<p>Step 7:-</p>

<p><code>sudo update-alternatives --install ''/usr/bin/javac&quot; &quot;javac&quot; &quot;/usr/local/java/bin/javac&quot; 1</code></p>

<p>Step 8:-</p>

<p><code>sudo update-alternatives --install ''/usr/bin/javaws&quot; &quot;javaws&quot; &quot;/usr/local/java/bin/javaws&quot; 1</code></p>

<p>Step 9 :-</p>

<p><code>sudo update-alternatives --set java /usr/local/java/bin/java</code></p>

<p><code>sudo update-alternatives --set javac /usr/local/java/bin/javac</code></p>

<p><code>sudo update-alternatives --set javaws /usr/local/java/bin/javaws</code></p>

<p><code>sudo vi  ~/.bashrc</code>
(Write at the end below script)</p>

<p>esc+o (to add new lines)</p>

<pre><code>JAVA_HOME=/usr/local/java/java
JRE_HOME=$JAVA_HOME/jre
PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin

export JAVA_HOME
export JRE_HOME
export PATH
</code></pre>

<p>:wq (To save the file and exit)</p>

<p>Step 10:- <code>source ~/.bashrc</code></p>

<p>Step 11:- <code>java -version</code></p>

<p>Output of above command should be :</p>

<pre><code>java version &quot;1.8.0_121&quot;
Java(TM) SE Runtime Environment (build 1.8.0_121-b13)
Java HotSpot(TM) Server VM (build 25.121-b13, mixed mode)
</code></pre>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://omennemo.github.io/tutorial/post/install-hadoop/">
        Installing Hadoop on the Ubuntu Guest
      </a>
    </h1>

    <span class="post-date">Thu, Apr 13, 2017</span>

    

<h2 id="configure-user">Configure User</h2>

<ul>
<li><p><strong>Login as Root:</strong></p>

<p><code>sudo su</code></p>

<p><code>whoami</code>   &ndash; should give root</p></li>

<li><p><strong>Adding a dedicated Hadoop system user called “hduser”</strong></p>

<p>We will use a dedicated Hadoop user account for running Hadoop. While that’s not required it is recommended because it helps to separate the Hadoop installation from other software applications and user accounts running on the same machines (this: security, permission and backups etc.)</p></li>

<li><p><strong>Create a group called hadoop</strong></p>

<p><code>sudo  addgroup hadoop</code></p></li>

<li><p><strong>Create User “hduser”</strong></p>

<p><code>sudo adduser hduser</code></p>

<p>It might ask to enter password 2 ties followed by some information, just press enter and Yes. We have given password hadoop</p></li>

<li><p><strong>Add hduser to hadoop group</strong></p>

<p><code>sudo adduser hduser hadoop</code></p>

<p>One line command for creating user and adding to a particular group :</p>

<p><code>sudo adduser –ingroup hadoop hduser</code></p></li>

<li><p><strong>Add the “hduser” to sudo-ers list so that hduser can do admin tasks</strong></p>

<p><code>sudo visudo</code></p>

<p>Add a line under</p>

<pre><code>##Allow member of group sudo to execute any command anywhere in the format.
    
hduser ALL=(ALL) ALL
</code></pre>

<p>Control+X yes and enter to save file. Logout and login as hduser</p></li>
</ul>

<h2 id="configure-ssh">Configure SSH</h2>

<p>Hadoop require SSH access to manage its nodes, i.e. remote machines plus your local machine if you want to use Hadoop on it (which is what we want to do in this class). For our single node setup of Hadoop, we therefore need to configure SSH access to localhost for the hduse user we created in the previous section.</p>

<ol>
<li><p><strong>Install ssh server on your machine</strong></p>

<p><code>sudo apt-get install openssh-server</code></p>

<p>If this did not work, then install openssh-server using Ubuntu software center by searching for openssh-server</p></li>

<li><p><strong>Generate SSH key for communication</strong></p>

<p><code>ssh-keygen</code></p>

<ul>
<li>Just press enter for whatever is asked.</li>
<li>Generate public/private rsa key pair</li>
<li>Enter file to save the key (/home/hduser/.ssh/id_rsa):</li>
<li>Created directory ‘/home/hduser/.ssh’</li>
<li>Your identification has been saved in /home/hduser/.ssh/id_rsa</li>
<li>Your public key has been saved in /home/hduser/.ssh/id_rsa.pub</li>
</ul></li>

<li><p><strong>Copy Public Key to Authorized_key file &amp; edit the permission</strong></p>

<p>Now copy the public key to the authorized_keys file, so that ssh shpuld not require password every time</p>

<p><code>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code></p>

<p>Change permission of the authorized_keys file to have all permission for hduser</p>

<p><code>chmod 700 ~/.ssh/authorized_keys</code></p></li>

<li><p><strong>Start SSH</strong></p>

<p>If ssh is not running, then run it by giving the below command</p>

<p><code>sudo /etc/init.d/ssh restart</code></p></li>

<li><p><strong>Test your SSH connectivity</strong></p>

<p><code>ssh localhost</code></p>

<p>Type yes, when asked for. You should be able to connect without password. If you are asked to enter password here, then something went wrong. Please check you previous steps.</p></li>

<li><p><strong>Disable IPV6</strong></p>

<p>Hadoop and IPV6 do not agree on the meaning of 0.0.0.0 address, thus it is advisable to disable IPV6 adding following lines at the end of <em>/etc/sysctl.conf</em></p>

<p><code>sudo nano /etc/sysctl.conf</code></p>

<pre><code>#disable ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.io.disable_ipv6 = 1
</code></pre></li>

<li><p><strong>Check if IPV6 is disable or not</strong></p>

<p><code>cat /proc/sys/net/ipv6/conf/all/disable_ipv6</code></p>

<p>It should show you <em>0</em>, after reboot it should show you <em>1</em></p></li>
</ol>

<h2 id="install-hadoop">Install Hadoop</h2>

<ol>
<li><p><strong>Download Hadoop</strong></p>

<p>Let us install hadoop 2.7.3 (hadoop-2.7.3.tar.gz). Download hadoop-2.7.3.tar.gz from <a href="http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz">apache</a> and save it in hduser/Desktop</p></li>

<li><p><strong>Move the zip file to /usr/local/</strong></p>

<p><code>sudo mv  ~/Desktop/hadoop-2.7.3.tar.gz /usr/local/</code></p>

<p><code>cd /usr/local</code></p>

<p><code>sudo tar –xvzf hadoop-2.7.3.tar.gz</code></p>

<p><code>sudo rm hadoop-2.7.3.tar.gz</code></p>

<p><code>sudo ln –s hadoop-2.7.3 hadoop</code></p>

<p><code>sudo chown –R hduser:hadoop hadoop-2.7.3</code></p>

<p><code>sudo chown –R hduser:hadoop hadoop</code></p>

<p><code>sudo chmod 777 hadoop-2.7.3</code></p></li>

<li><p><strong>Edit hadoop-env.sh and configure Java</strong></p>

<p>Add the following to <em>/usr/local/hadoop/etc/hadoop/hadoop-env.sh</em> by <strong>removing</strong></p>

<pre><code>export JAVA_HOME=${JAVA_HOME}
</code></pre>

<p>Editing hadoop-env.sh and <strong>adding the lines below</strong></p>

<p><code>sudo nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh</code></p>

<pre><code>export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true
export HADOOP_HOME_WARN_SUPPRESS=”TRUE”
export JAVA_HOME=/usr/local/java
</code></pre></li>

<li><p><strong>Update ~/.bashrc</strong></p>

<pre><code>#Set Hadoop-related environment variables
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_PREFIX=/usr/local/hadoop
export HADOOP_MAPRED_HOME=${HADOOP_HOME}
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=${HADOOP_HOME}
export HADOOP_YARN_HOME=${HADOOP_HOME}
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop

#Native Path

export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native
export HADOOP_OPTS=”-Djava.library.path=$HADOOP_PREFIX/lib”

#some convenient aliases and functions for running hadoop related command

JAVA_HOME=/usr/local/java
JRE_HOME=$JAVA_HOME/jre
PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
export JAVA_HOME
export JRE_HOME
export PATH

export PATH=$PATH:$HADOOP_HOME/bin:$PATH:$HADOOP_HOME/sbin
</code></pre>

<p><code>source ~/.bashrc</code></p></li>

<li><p><strong>Create a temporary directory which will be used as base location for DFS</strong></p>

<p><code>sudo mkdir –p /app/hadoop/tmp</code></p>

<p><code>sudo chown –R hduser:hadoop /app/hadoop/tmp</code></p>

<p><code>sudo chmod –R 777 /app/hadoop/tmp</code></p></li>

<li><p><strong>Update yarn-site.xml</strong></p>

<p><code>nano /usr/local/hadoop/etc/hadoop/yarn-site.xml</code></p>

<pre><code>&lt;property&gt;
	&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
	&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
	&lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
	&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
&lt;/property&gt;
</code></pre></li>

<li><p><strong>Update core-site.xml</strong></p>

<p><code>nano /usr/local/hadoop/etc/hadoop/core-site.xml</code></p>

<pre><code>&lt;property&gt;
    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
    &lt;value&gt;/app/hadoop/tmp&lt;/value&gt;
    &lt;description&gt;A base for other temporary directory.&lt;/description&gt;
&lt;/property&gt;
    
&lt;property&gt;
    &lt;name&gt;fs.default.name&lt;/name&gt;
    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
    &lt;description&gt;The name of the default file system.
A URI whose scheme and authority determine the FileSystem implementation. The uri’s scheme determine the config property (fs.SCHEME.impl) naming the    FileSystem implementation class. The uri’s authority is used to determine the host, port, etc. for a filesystem
&lt;/description&gt;
&lt;/property&gt;
</code></pre></li>

<li><p><strong>Create mapred-site.xml from mapred-site.xml.template</strong></p>

<p><code>cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml</code></p>

<p><code>nano /usr/local/hadoop/etc/hadoop/mapred-site.xml</code></p>

<pre><code>&lt;property&gt;
    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
    &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</code></pre></li>

<li><p><strong>Create some directory</strong></p>

<p><code>sudo mkdir –p /usr/local/hadoop/yarn_data/hdfs/namenode</code></p>

<p><code>sudo mkdir –p /usr/local/hadoop/yarn_data/hdfs/datanode</code></p>

<p><code>sudo chmod 777 /usr/local/hadoop/yarn_data/hdfs/namenode</code></p>

<p><code>sudo chmod 777 /usr/local/hadoop/yarn_data/hdfs/datanode</code></p>

<p><code>sudo chown –R hduser:hadoop /usr/local/hadoop/yarn_data/hdfs/namenode</code></p>

<p><code>sudo chown –R hduser:hadoop /usr/local/hadoop/yarn_data/hdfs/datanode</code></p></li>

<li><p><strong>Update hdfs-site.html</strong></p>

<p><code>nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml</code></p>

<pre><code>&lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
    
&lt;property&gt;
    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
    &lt;value&gt;file:/usr/local/hadoop/yarn_data/hdfs/namenode &lt;/value&gt;
&lt;/property&gt;
    
&lt;property&gt;
    &lt;name&gt;dfs.datanode.name.dir&lt;/name&gt;
    &lt;value&gt;file:/usr/local/hadoop/yarn_data/hdfs/datanode &lt;/value&gt;
&lt;/property&gt;
</code></pre></li>

<li><p><strong>Format your namenode</strong></p>

<p><code>hadoop namenode –format</code></p></li>

<li><p><strong>Start your single node cluster</strong></p>

<p><code>start-dfs.sh</code></p>

<p><code>start-yarn.sh</code></p>

<p><code>jps</code></p></li>
</ol>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://omennemo.github.io/tutorial/post/install-multinode/">
        Installing a multinode cluster
      </a>
    </h1>

    <span class="post-date">Thu, Apr 13, 2017</span>

    

<h1 id="create-machines">Create machines</h1>

<ol>
<li>Create 3 nodes</li>
<li>Load VMs (NAME: master , datanode1, datanode2)</li>
<li>Check if nodes are ping-able (with IP address – use command <code>ifconfig</code> to find IP Address)</li>

<li><p>Change the hostname</p>

<p><code>sudo nano /etc/hostname</code></p>

<p>Replace with  (master, datanode1, datanode2)</p></li>

<li><p>Update host file and update with IP address</p>

<p><code>sudo nano /etc/hosts</code></p>

<p>Delete Ubuntu line and add these with the appropriate IP addresses</p>

<pre><code>192.168.72.X    master
192.168.72.Y    datanode1
192.168.72.Z    datanode2
</code></pre></li>

<li><p>Reboot</p></li>

<li><p>Ping with hostname and check reachability</p></li>

<li><p>Check ssh login to other hosts (without password)</p></li>
</ol>

<ul>
<li><p><strong>core-site.xml (both master and datanodes</strong></p>

<p>Delete 4 lines for property name <code>&lt;hadoop.tmp.dir&gt;</code>
and change <code>localhost:9000</code> to <code>master:9000</code> in property name: <code>fs.default.name</code></p></li>

<li><p><strong>hdfs-site.xml (both master and datanodes)</strong></p>

<p>Change the replication factor from 1 to 2 (or n as appropriate) in all nodes and delete datanode property from master node and delete namenode property from all data nodes</p></li>

<li><p><strong>yarn-site.xml (both master and datanodes)</strong></p>

<pre><code>&lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
        &lt;value&gt;master:8025&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
        &lt;value&gt;master:8030&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
        &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
        &lt;value&gt;master:8050&lt;/value&gt;
&lt;/property&gt;
</code></pre></li>

<li><p><strong>mapred-site.xml (both master and datanodes)</strong></p>

<pre><code>&lt;property&gt;
        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;
        &lt;value&gt;master:10020&lt;/value&gt;
        &lt;description&gt;
    Host and port for Job History Server (default)
    0.0.0.0:10020
        &lt;/description&gt;
&lt;/property&gt;
</code></pre></li>

<li><p><strong>master only configuration:</strong></p>

<p><code>sudo nano /usr/local/hadoop/etc/hadoop/slaves</code>
    and add datanodes names (like datanode1 and datanode2 in our case)</p>

<p><code>sudo nano /usr/local/hadoop/etc/hadoop/masters</code>
    and add master names (like master in our case)</p>

<p><code>sudo rm -rf /usr/local/hadoop/yarn_data/hdfs/namenode</code></p>

<p><code>sudo mkdir -p /usr/local/hadoop/yarn_data/hdfs/namenode</code></p>

<p><code>sudo chown -R hduser:hadoop /usr/local/hadoop/yarn_data</code></p>

<p><code>sudo chmod 777 /usr/local/hadoop/yarn_data/hdfs/namenode</code></p></li>

<li><p><strong>Run on all datanodes</strong></p>

<p><code>sudo rm -rf /usr/local/hadoop/yarn_data/hdfs/datanode</code></p>

<p><code>sudo mkdir -p /usr/local/hadoop/yarn_data/hdfs/datanode</code></p>

<p><code>sudo chown -R hduser:hadoop /usr/local/hadoop/yarn_data</code></p>

<p><code>sudo chmod 777 /usr/local/hadoop/yarn_data/hdfs/datanode</code></p></li>

<li><p><strong>Format namenode in master</strong></p>

<p><code>hadoop namenode -format</code></p></li>

<li><p><strong>Start servers from scripts in master</strong></p>

<p><code>start-dfs.sh</code></p>

<p><code>start-yarn.sh</code></p>

<p><code>jps</code></p></li>

<li><p><strong>Status can be checked using the following methods</strong></p>

<p><a href="http://master:8088">http://master:8088</a></p>

<p><a href="http://master:50070">http://master:50070</a></p>

<p><strong>OR</strong></p>

<p><code>hdfs dfsadmin -report</code></p>

<p>check the following path : <code>/usr/local/Hadoop/logs</code></p></li>
</ul>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="https://omennemo.github.io/tutorial/post/install-hive/">
        Installing Hive
      </a>
    </h1>

    <span class="post-date">Thu, Apr 13, 2017</span>

    

<h1 id="guide-to-installing-hive-on-a-hadoop-cluster">Guide to installing Hive on a hadoop cluster</h1>

<ul>
<li><p>Create a folder called ecosystem at ~</p>

<p><code>mkdir ecosystem</code></p>

<p><code>cd ecosystem</code></p></li>

<li><p>Get the Hive installation file</p>

<p><code>wget http://mirror.fibergrid.in/apache/hive/hive-2.1.1/apache-hive-2.1.1-bin.tar.gz</code></p>

<p>If this command doesn&rsquo;t work, please search for the installation zip file and use the same command to download the file.</p></li>

<li><p>Un-tar the file</p>

<p><code>tar -xvzf apache-hive-2.1.1-bin.tar.gz</code></p></li>

<li><p>Remove the tar file once the file is extracted properly</p>

<p><code>rm apache-hive-2.1.1-bin.tar.gz</code></p></li>

<li><p>Create a soft link to the file</p>

<p><code>ln –s apache-hive-2.1.1-bin hive</code></p></li>

<li><p>Update the .bashrc file</p>

<p><code>nano /home/hduser/.bashrc</code></p>

<p>Add the below block to the end of the file.</p>

<pre><code>export HIVE_PATH=/home/hduser/ecosystem/hive
export PATH=$PATH:$HIVE_PATH/bin
</code></pre></li>

<li><p>Run <code>source ~/.bashrc</code></p></li>

<li><p>Start hadoop cluster (with <code>start-dfs.sh</code> and <code>start-yarn.sh</code>)</p></li>

<li><p>Creating Hive Directories</p>

<p><code>hdfs dfs -mkdir -p /user/hive/warehouse</code></p>

<p><code>hdfs dfs -mkdir -p /tmp/hive</code></p>

<p><code>hdfs dfs -chmod 777 /tmp</code></p>

<p><code>hdfs dfs -chmod 777 /user/hive/warehouse</code></p>

<p><code>hdfs dfs -chmod 777 /tmp/hive</code></p></li>

<li><p>Pre-emptively remove the problematic .jar file</p>

<p><code>rm /home/hduser/ecosystem/hive/lib/log4j-slf4j-impl-2.4.1.jar</code></p></li>

<li><p>Run schemaTool</p>

<p><code>schematool -initSchema -dbType derby</code></p></li>

<li><p>Bring up the hive console to query the database</p>

<p><code>hive</code></p></li>
</ul>

<h2 id="crud-in-hive">CRUD in Hive</h2>

<ul>
<li><p>Create Employee Table</p>

<pre><code>CREATE TABLE EMPLOYEE
(
    EMPNO INT,
    NAME STRING,
    JOB STRING,
    BOSS INT,
    HIREDATE TIMESTAMP,
    SALARY FLOAT,
    COMM FLOAT,
    DEPENO INT
)
ROW FORMAT DELIMITED
FIELDS TERMNINATED BY ‘,’
;
</code></pre></li>

<li><p>Create file in local file system /home/hduser/empData.txt with the below contents</p>

<pre><code>7839,KING,PRESIDENT,0,1981-11-17 23:59:00,5000,0,10
7566,JONES,MANAGER,7839,1981-04-02 23:59:00,2975,20
7788,SCOTT,ANALYST,7566, 1982-12-09 23:59:00,3000,0,20
7876,ADAMS,CLERK,7788,1983-01-12 23:59:00,1100,0,20
7902,FORD,ANALYST,7566,1981-12-03 23:59:00,3000,0,20
7369,SMITH,CLERK,7902,1980-12-17 23:59:00,800,0,20
7698,BLACK,MANAGER,7839,1981-05-01 23:59:00,2850,0,30
7499,ALLEN,SALESMAN,7698,1981-02-20 23:59:00,1600,300,30
7521,WARD,SALESMAN,7698,1981-02-22 23:59:00,1250,500,30
7654,MARTIN,SALESMAN,7698,1981-09-28 23:59:00,1250,1400,30
7844,TURNER,SALESMAN,7698,1981-09-08 23:59:00,1500,0,30
7900,JAMES,CLERK,7698,1981-12-03 23:59:00,950,0,10
7782,CLARK,MANAGER,7839,1981-06-09 23:59:00,2450,0,10
7934,MILLER,CLERK,7782,1982-01-23 23:59:00,1300,0,10
</code></pre></li>

<li><p>Load data into Employee table</p>

<p><code>LOAD DATA LOCAL INPATH ‘/home/hduser/empData.txt’ OVERWRITE INTO TABLE EMPLOYEE;</code></p></li>

<li><p>Execute query and check</p>

<pre><code>SELECT * FROM EMPLOYEE;
    
Select * from EMPLOYEE e where e.salary in (select max(salary) from EMPLOYEE);
</code></pre></li>
</ul>

  </div>
  
</div>
</div>

  </body>
</html>
