	<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.20" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  
  <title>Installing Hadoop on the Ubuntu Guest &middot; My Big Data Collection</title>
  

  
  <link rel="stylesheet" href="https://omennemo.github.io/tutorial/css/poole.css">
  <link rel="stylesheet" href="https://omennemo.github.io/tutorial/css/syntax.css">
  <link rel="stylesheet" href="https://omennemo.github.io/tutorial/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="My Big Data Collection" />
</head>

	<body class=" ">
		<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://omennemo.github.io/tutorial/"><h1>My Big Data Collection</h1></a>
      <p class="lead">
      An elegant open source and mobile first theme for <a href="http://hugo.spf13.com">hugo</a> made by <a href="http://twitter.com/mdo">@mdo</a>. Originally made for Jekyll.
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="/">Home</a> </li>
      
    </ul>

    <p>&copy; 2017. All rights reserved. </p>
  </div>
</div>


		<div class="content container">
			<div class="post">
			 	<h1>Installing Hadoop on the Ubuntu Guest</h1>
			  <span class="post-date">Thu, Apr 13, 2017</span>
			      

<h2 id="configure-user">Configure User</h2>

<ul>
<li><p><strong>Login as Root:</strong></p>

<p><code>sudo su</code></p>

<p><code>whoami</code>   &ndash; should give root</p></li>

<li><p><strong>Adding a dedicated Hadoop system user called “hduser”</strong></p>

<p>We will use a dedicated Hadoop user account for running Hadoop. While that’s not required it is recommended because it helps to separate the Hadoop installation from other software applications and user accounts running on the same machines (this: security, permission and backups etc.)</p></li>

<li><p><strong>Create a group called hadoop</strong></p>

<p><code>sudo  addgroup hadoop</code></p></li>

<li><p><strong>Create User “hduser”</strong></p>

<p><code>sudo adduser hduser</code></p>

<p>It might ask to enter password 2 ties followed by some information, just press enter and Yes. We have given password hadoop</p></li>

<li><p><strong>Add hduser to hadoop group</strong></p>

<p><code>sudo adduser hduser hadoop</code></p>

<p>One line command for creating user and adding to a particular group :</p>

<p><code>sudo adduser –ingroup hadoop hduser</code></p></li>

<li><p><strong>Add the “hduser” to sudo-ers list so that hduser can do admin tasks</strong></p>

<p><code>sudo visudo</code></p>

<p>Add a line under</p>

<pre><code>##Allow member of group sudo to execute any command anywhere in the format.
    
hduser ALL=(ALL) ALL
</code></pre>

<p>Control+X yes and enter to save file. Logout and login as hduser</p></li>
</ul>

<h2 id="configure-ssh">Configure SSH</h2>

<p>Hadoop require SSH access to manage its nodes, i.e. remote machines plus your local machine if you want to use Hadoop on it (which is what we want to do in this class). For our single node setup of Hadoop, we therefore need to configure SSH access to localhost for the hduse user we created in the previous section.</p>

<ol>
<li><p><strong>Install ssh server on your machine</strong></p>

<p><code>sudo apt-get install openssh-server</code></p>

<p>If this did not work, then install openssh-server using Ubuntu software center by searching for openssh-server</p></li>

<li><p><strong>Generate SSH key for communication</strong></p>

<p><code>ssh-keygen</code></p>

<ul>
<li>Just press enter for whatever is asked.</li>
<li>Generate public/private rsa key pair</li>
<li>Enter file to save the key (/home/hduser/.ssh/id_rsa):</li>
<li>Created directory ‘/home/hduser/.ssh’</li>
<li>Your identification has been saved in /home/hduser/.ssh/id_rsa</li>
<li>Your public key has been saved in /home/hduser/.ssh/id_rsa.pub</li>
</ul></li>

<li><p><strong>Copy Public Key to Authorized_key file &amp; edit the permission</strong></p>

<p>Now copy the public key to the authorized_keys file, so that ssh shpuld not require password every time</p>

<p><code>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</code></p>

<p>Change permission of the authorized_keys file to have all permission for hduser</p>

<p><code>chmod 700 ~/.ssh/authorized_keys</code></p></li>

<li><p><strong>Start SSH</strong></p>

<p>If ssh is not running, then run it by giving the below command</p>

<p><code>sudo /etc/init.d/ssh restart</code></p></li>

<li><p><strong>Test your SSH connectivity</strong></p>

<p><code>ssh localhost</code></p>

<p>Type yes, when asked for. You should be able to connect without password. If you are asked to enter password here, then something went wrong. Please check you previous steps.</p></li>

<li><p><strong>Disable IPV6</strong></p>

<p>Hadoop and IPV6 do not agree on the meaning of 0.0.0.0 address, thus it is advisable to disable IPV6 adding following lines at the end of <em>/etc/sysctl.conf</em></p>

<p><code>sudo nano /etc/sysctl.conf</code></p>

<pre><code>#disable ipv6
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.io.disable_ipv6 = 1
</code></pre></li>

<li><p><strong>Check if IPV6 is disable or not</strong></p>

<p><code>cat /proc/sys/net/ipv6/conf/all/disable_ipv6</code></p>

<p>It should show you <em>0</em>, after reboot it should show you <em>1</em></p></li>
</ol>

<h2 id="install-hadoop">Install Hadoop</h2>

<ol>
<li><p><strong>Download Hadoop</strong></p>

<p>Let us install hadoop 2.7.3 (hadoop-2.7.3.tar.gz). Download hadoop-2.7.3.tar.gz from <a href="http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz">apache</a> and save it in hduser/Desktop</p></li>

<li><p><strong>Move the zip file to /usr/local/</strong></p>

<p><code>sudo mv  ~/Desktop/hadoop-2.7.3.tar.gz /usr/local/</code></p>

<p><code>cd /usr/local</code></p>

<p><code>sudo tar –xvzf hadoop-2.7.3.tar.gz</code></p>

<p><code>sudo rm hadoop-2.7.3.tar.gz</code></p>

<p><code>sudo ln –s hadoop-2.7.3 hadoop</code></p>

<p><code>sudo chown –R hduser:hadoop hadoop-2.7.3</code></p>

<p><code>sudo chown –R hduser:hadoop hadoop</code></p>

<p><code>sudo chmod 777 hadoop-2.7.3</code></p></li>

<li><p><strong>Edit hadoop-env.sh and configure Java</strong></p>

<p>Add the following to <em>/usr/local/hadoop/etc/hadoop/hadoop-env.sh</em> by <strong>removing</strong></p>

<pre><code>export JAVA_HOME=${JAVA_HOME}
</code></pre>

<p>Editing hadoop-env.sh and <strong>adding the lines below</strong></p>

<p><code>sudo nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh</code></p>

<pre><code>export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true
export HADOOP_HOME_WARN_SUPPRESS=”TRUE”
export JAVA_HOME=/usr/local/java
</code></pre></li>

<li><p><strong>Update ~/.bashrc</strong></p>

<pre><code>#Set Hadoop-related environment variables
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_PREFIX=/usr/local/hadoop
export HADOOP_MAPRED_HOME=${HADOOP_HOME}
export HADOOP_COMMON_HOME=${HADOOP_HOME}
export HADOOP_HDFS_HOME=${HADOOP_HOME}
export HADOOP_YARN_HOME=${HADOOP_HOME}
export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop

#Native Path

export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_PREFIX}/lib/native
export HADOOP_OPTS=”-Djava.library.path=$HADOOP_PREFIX/lib”

#some convenient aliases and functions for running hadoop related command

JAVA_HOME=/usr/local/java
JRE_HOME=$JAVA_HOME/jre
PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin
export JAVA_HOME
export JRE_HOME
export PATH

export PATH=$PATH:$HADOOP_HOME/bin:$PATH:$HADOOP_HOME/sbin
</code></pre>

<p><code>source ~/.bashrc</code></p></li>

<li><p><strong>Create a temporary directory which will be used as base location for DFS</strong></p>

<p><code>sudo mkdir –p /app/hadoop/tmp</code></p>

<p><code>sudo chown –R hduser:hadoop /app/hadoop/tmp</code></p>

<p><code>sudo chmod –R 777 /app/hadoop/tmp</code></p></li>

<li><p><strong>Update yarn-site.xml</strong></p>

<p><code>nano /usr/local/hadoop/etc/hadoop/yarn-site.xml</code></p>

<pre><code>&lt;property&gt;
	&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
	&lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
	&lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
	&lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
&lt;/property&gt;
</code></pre></li>

<li><p><strong>Update core-site.xml</strong></p>

<p><code>nano /usr/local/hadoop/etc/hadoop/core-site.xml</code></p>

<pre><code>&lt;property&gt;
    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
    &lt;value&gt;/app/hadoop/tmp&lt;/value&gt;
    &lt;description&gt;A base for other temporary directory.&lt;/description&gt;
&lt;/property&gt;
    
&lt;property&gt;
    &lt;name&gt;fs.default.name&lt;/name&gt;
    &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
    &lt;description&gt;The name of the default file system.
A URI whose scheme and authority determine the FileSystem implementation. The uri’s scheme determine the config property (fs.SCHEME.impl) naming the    FileSystem implementation class. The uri’s authority is used to determine the host, port, etc. for a filesystem
&lt;/description&gt;
&lt;/property&gt;
</code></pre></li>

<li><p><strong>Create mapred-site.xml from mapred-site.xml.template</strong></p>

<p><code>cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml</code></p>

<p><code>nano /usr/local/hadoop/etc/hadoop/mapred-site.xml</code></p>

<pre><code>&lt;property&gt;
    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
    &lt;value&gt;yarn&lt;/value&gt;
&lt;/property&gt;
</code></pre></li>

<li><p><strong>Create some directory</strong></p>

<p><code>sudo mkdir –p /usr/local/hadoop/yarn_data/hdfs/namenode</code></p>

<p><code>sudo mkdir –p /usr/local/hadoop/yarn_data/hdfs/datanode</code></p>

<p><code>sudo chmod 777 /usr/local/hadoop/yarn_data/hdfs/namenode</code></p>

<p><code>sudo chmod 777 /usr/local/hadoop/yarn_data/hdfs/datanode</code></p>

<p><code>sudo chown –R hduser:hadoop /usr/local/hadoop/yarn_data/hdfs/namenode</code></p>

<p><code>sudo chown –R hduser:hadoop /usr/local/hadoop/yarn_data/hdfs/datanode</code></p></li>

<li><p><strong>Update hdfs-site.html</strong></p>

<p><code>nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml</code></p>

<pre><code>&lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
&lt;/property&gt;
    
&lt;property&gt;
    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
    &lt;value&gt;file:/usr/local/hadoop/yarn_data/hdfs/namenode &lt;/value&gt;
&lt;/property&gt;
    
&lt;property&gt;
    &lt;name&gt;dfs.datanode.name.dir&lt;/name&gt;
    &lt;value&gt;file:/usr/local/hadoop/yarn_data/hdfs/datanode &lt;/value&gt;
&lt;/property&gt;
</code></pre></li>

<li><p><strong>Format your namenode</strong></p>

<p><code>hadoop namenode –format</code></p></li>

<li><p><strong>Start your single node cluster</strong></p>

<p><code>start-dfs.sh</code></p>

<p><code>start-yarn.sh</code></p>

<p><code>jps</code></p></li>
</ol>

			</div>

			
		</div>

  </body>
</html>
